{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import silero_vad\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size = 1.8070078125 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Freja Holthaus/.cache\\torch\\hub\\snakers4_silero-vad_master\n"
     ]
    }
   ],
   "source": [
    "T.set_num_threads(1)\n",
    "vad_model, vad_utils = T.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad')\n",
    "print(f\"model size = {sum([p.numel() for p in vad_model.parameters()]) * 4 / 1.024e6} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71760])\n"
     ]
    }
   ],
   "source": [
    "test_audio_file = \"C:\\\\datasets\\\\OpenSpeechAndLanguageResource\\\\test-clean\\\\test-clean\\\\61\\\\70970\\\\61-70970-0007.flac\"\n",
    "test_audio = silero_vad.read_audio(test_audio_file)\n",
    "print(test_audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from device_capture_system import deviceIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Callable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;129m@T\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_speech_timestamps\u001b[39m(\n\u001b[0;32m      3\u001b[0m     audio: T\u001b[38;5;241m.\u001b[39mTensor, \n\u001b[0;32m      4\u001b[0m     model,\n\u001b[0;32m      5\u001b[0m     sampling_rate: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16000\u001b[39m,\n\u001b[0;32m      6\u001b[0m     threshold: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m      7\u001b[0m     min_speech_duration_ms: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m250\u001b[39m,\n\u001b[0;32m      8\u001b[0m     max_speech_duration_s: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      9\u001b[0m     min_silence_duration_ms: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     10\u001b[0m     speech_pad_ms: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m     11\u001b[0m     return_seconds: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m     visualize_probs: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m---> 13\u001b[0m     progress_tracking_callback: \u001b[43mCallable\u001b[49m[[\u001b[38;5;28mfloat\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m     neg_threshold: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    This method is used for splitting long audios into speech chunks using silero VAD\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m        list containing ends and beginnings of speech chunks (samples or seconds based on return_seconds)\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(audio) \u001b[38;5;241m==\u001b[39m T\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio must be a tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Callable' is not defined"
     ]
    }
   ],
   "source": [
    "@T.no_grad()\n",
    "def get_speech_timestamps(\n",
    "    audio: T.Tensor, \n",
    "    model: T.nn.Module,\n",
    "    window_siwze = 512):\n",
    "\n",
    "    # sampling_rate: int = 16000,\n",
    "    # threshold: float = 0.5,\n",
    "    # min_speech_duration_ms: int = 250,\n",
    "    # max_speech_duration_s: float = float('inf'),\n",
    "    # min_silence_duration_ms: int = 100,\n",
    "    # speech_pad_ms: int = 30,\n",
    "    # return_seconds: bool = False,\n",
    "    # visualize_probs: bool = False,\n",
    "    # progress_tracking_callback: Callable[[float], None] = None,\n",
    "    # neg_threshold: float = None):\n",
    "\n",
    "    \"\"\"\n",
    "        ! This code is taken from https://github.com/snakers4/silero-vad and changed\n",
    "\n",
    "        ! sample rate has to be 16000, upsample or downsample the audio to 16000\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio: torch.Tensor, one dimensional\n",
    "        One dimensional float torch.Tensor, other types are casted to torch if possible\n",
    "\n",
    "    model: preloaded .jit/.onnx silero VAD model\n",
    "\n",
    "    threshold: float (default - 0.5)\n",
    "        Speech threshold. Silero VAD outputs speech probabilities for each audio chunk, probabilities ABOVE this value are considered as SPEECH.\n",
    "        It is better to tune this parameter for each dataset separately, but \"lazy\" 0.5 is pretty good for most datasets.\n",
    "\n",
    "    sampling_rate: int (default - 16000)\n",
    "        Currently silero VAD models support 8000 and 16000 (or multiply of 16000) sample rates\n",
    "\n",
    "    min_speech_duration_ms: int (default - 250 milliseconds)\n",
    "        Final speech chunks shorter min_speech_duration_ms are thrown out\n",
    "\n",
    "    max_speech_duration_s: int (default -  inf)\n",
    "        Maximum duration of speech chunks in seconds\n",
    "        Chunks longer than max_speech_duration_s will be split at the timestamp of the last silence that lasts more than 100ms (if any), to prevent agressive cutting.\n",
    "        Otherwise, they will be split aggressively just before max_speech_duration_s.\n",
    "\n",
    "    min_silence_duration_ms: int (default - 100 milliseconds)\n",
    "        In the end of each speech chunk wait for min_silence_duration_ms before separating it\n",
    "\n",
    "    speech_pad_ms: int (default - 30 milliseconds)\n",
    "        Final speech chunks are padded by speech_pad_ms each side\n",
    "\n",
    "    return_seconds: bool (default - False)\n",
    "        whether return timestamps in seconds (default - samples)\n",
    "\n",
    "    visualize_probs: bool (default - False)\n",
    "        whether draw prob hist or not\n",
    "\n",
    "    progress_tracking_callback: Callable[[float], None] (default - None)\n",
    "        callback function taking progress in percents as an argument\n",
    "\n",
    "    neg_threshold: float (default = threshold - 0.15)\n",
    "        Negative threshold (noise or exit threshold). If model's current state is SPEECH, values BELOW this value are considered as NON-SPEECH.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    speeches: list of dicts\n",
    "        list containing ends and beginnings of speech chunks (samples or seconds based on return_seconds)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    assert type(audio) == T.Tensor, \"Audio must be a tensor\"\n",
    "    assert audio.dim() == 1, \"Audio must be one-dimensional\"\n",
    "\n",
    "\n",
    "    model.reset_states()\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "        Variable Initialization:\n",
    "\n",
    "        min_speech_samples, \n",
    "        speech_pad_samples, \n",
    "        max_speech_samples, \n",
    "        min_silence_samples, \n",
    "        min_silence_samples_at_max_speech: \n",
    "        Speech Probability Calculation:\n",
    "\n",
    "        speech_probs: An empty list to store the speech probabilities for each audio chunk.\n",
    "        The for loop iterates over the audio in chunks of window_siwze. Each chunk is padded if necessary and passed to the model to get the speech probability, which is then appended to speech_probs.\n",
    "        Progress is calculated and sent to a callback function if provided.\n",
    "        Speech Segmentation:\n",
    "\n",
    "        triggered, speeches, current_speech: Variables to manage the state of speech detection. triggered indicates if speech is currently being detected. speeches is a list to store detected speech segments. current_speech holds the current speech segment being processed.\n",
    "        neg_threshold: A threshold for detecting the end of speech, set to a value slightly lower than the main threshold.\n",
    "        temp_end, prev_end, next_start: Variables to manage potential segment ends and tolerate some silence.\n",
    "        Processing Speech Probabilities:\n",
    "\n",
    "        The for loop iterates over speech_probs to detect speech segments based on the thresholds and durations.\n",
    "        If speech probability exceeds the threshold and triggered is False, a new speech segment starts.\n",
    "        If the speech segment exceeds max_speech_samples, it is split, and the current segment is saved.\n",
    "        If speech probability drops below neg_threshold, potential segment ends are managed, and segments are saved if they meet the minimum speech duration.\n",
    "        Final Adjustments to Speech Segments:\n",
    "\n",
    "        If there is an ongoing speech segment at the end of the audio, it is finalized and added to speeches.\n",
    "        The for loop adjusts the start and end times of speech segments to include padding and manage overlaps.\n",
    "        Return Format Adjustments:\n",
    "\n",
    "        If return_seconds is True, the start and end times of speech segments are converted from samples to seconds.\n",
    "        If step is greater than 1, the start and end times are multiplied by step.\n",
    "        Visualization:\n",
    "\n",
    "        If visualize_probs is True, a visualization of the speech probabilities is created.\n",
    "        Return:\n",
    "\n",
    "        The function returns the list of detected speech segments (speeches).\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "        These variables are calculated based on the sampling rate and various duration parameters. \n",
    "        They represent the minimum number of samples for speech, \n",
    "        padding samples, maximum number of samples for speech, \n",
    "        minimum number of samples for silence, \n",
    "        and minimum silence samples at maximum speech, \n",
    "        respectively.\n",
    "    \"\"\"\n",
    "    min_speech_samples = sampling_rate * min_speech_duration_ms / 1000\n",
    "    speech_pad_samples = sampling_rate * speech_pad_ms / 1000\n",
    "    max_speech_samples = sampling_rate * max_speech_duration_s - window_siwze - 2 * speech_pad_samples\n",
    "    min_silence_samples = sampling_rate * min_silence_duration_ms / 1000\n",
    "    min_silence_samples_at_max_speech = sampling_rate * 98 / 1000\n",
    "\n",
    "    audio_length_samples = len(audio)\n",
    "\n",
    "    speech_probs = []\n",
    "    for current_start_sample in range(0, audio_length_samples, window_siwze):\n",
    "        chunk = audio[current_start_sample: current_start_sample + window_siwze]\n",
    "        if len(chunk) < window_siwze:\n",
    "            chunk = torch.nn.functional.pad(chunk, (0, int(window_siwze - len(chunk))))\n",
    "        speech_prob = model(chunk, sampling_rate).item()\n",
    "        speech_probs.append(speech_prob)\n",
    "        # caculate progress and seng it to callback function\n",
    "        progress = current_start_sample + window_siwze\n",
    "        if progress > audio_length_samples:\n",
    "            progress = audio_length_samples\n",
    "        progress_percent = (progress / audio_length_samples) * 100\n",
    "        if progress_tracking_callback:\n",
    "            progress_tracking_callback(progress_percent)\n",
    "\n",
    "    triggered = False\n",
    "    speeches = []\n",
    "    current_speech = {}\n",
    "\n",
    "    if neg_threshold is None:\n",
    "        neg_threshold = max(threshold - 0.15, 0.01)\n",
    "    temp_end = 0  # to save potential segment end (and tolerate some silence)\n",
    "    prev_end = next_start = 0  # to save potential segment limits in case of maximum segment size reached\n",
    "\n",
    "    for i, speech_prob in enumerate(speech_probs):\n",
    "        if (speech_prob >= threshold) and temp_end:\n",
    "            temp_end = 0\n",
    "            if next_start < prev_end:\n",
    "                next_start = window_siwze * i\n",
    "\n",
    "        if (speech_prob >= threshold) and not triggered:\n",
    "            triggered = True\n",
    "            current_speech['start'] = window_siwze * i\n",
    "            continue\n",
    "\n",
    "        if triggered and (window_siwze * i) - current_speech['start'] > max_speech_samples:\n",
    "            if prev_end:\n",
    "                current_speech['end'] = prev_end\n",
    "                speeches.append(current_speech)\n",
    "                current_speech = {}\n",
    "                if next_start < prev_end:  # previously reached silence (< neg_thres) and is still not speech (< thres)\n",
    "                    triggered = False\n",
    "                else:\n",
    "                    current_speech['start'] = next_start\n",
    "                prev_end = next_start = temp_end = 0\n",
    "            else:\n",
    "                current_speech['end'] = window_siwze * i\n",
    "                speeches.append(current_speech)\n",
    "                current_speech = {}\n",
    "                prev_end = next_start = temp_end = 0\n",
    "                triggered = False\n",
    "                continue\n",
    "\n",
    "        if (speech_prob < neg_threshold) and triggered:\n",
    "            if not temp_end:\n",
    "                temp_end = window_siwze * i\n",
    "            if ((window_siwze * i) - temp_end) > min_silence_samples_at_max_speech:  # condition to avoid cutting in very short silence\n",
    "                prev_end = temp_end\n",
    "            if (window_siwze * i) - temp_end < min_silence_samples:\n",
    "                continue\n",
    "            else:\n",
    "                current_speech['end'] = temp_end\n",
    "                if (current_speech['end'] - current_speech['start']) > min_speech_samples:\n",
    "                    speeches.append(current_speech)\n",
    "                current_speech = {}\n",
    "                prev_end = next_start = temp_end = 0\n",
    "                triggered = False\n",
    "                continue\n",
    "\n",
    "    if current_speech and (audio_length_samples - current_speech['start']) > min_speech_samples:\n",
    "        current_speech['end'] = audio_length_samples\n",
    "        speeches.append(current_speech)\n",
    "\n",
    "    for i, speech in enumerate(speeches):\n",
    "        if i == 0:\n",
    "            speech['start'] = int(max(0, speech['start'] - speech_pad_samples))\n",
    "        if i != len(speeches) - 1:\n",
    "            silence_duration = speeches[i+1]['start'] - speech['end']\n",
    "            if silence_duration < 2 * speech_pad_samples:\n",
    "                speech['end'] += int(silence_duration // 2)\n",
    "                speeches[i+1]['start'] = int(max(0, speeches[i+1]['start'] - silence_duration // 2))\n",
    "            else:\n",
    "                speech['end'] = int(min(audio_length_samples, speech['end'] + speech_pad_samples))\n",
    "                speeches[i+1]['start'] = int(max(0, speeches[i+1]['start'] - speech_pad_samples))\n",
    "        else:\n",
    "            speech['end'] = int(min(audio_length_samples, speech['end'] + speech_pad_samples))\n",
    "\n",
    "    if return_seconds:\n",
    "        audio_length_seconds = audio_length_samples / sampling_rate\n",
    "        for speech_dict in speeches:\n",
    "            speech_dict['start'] = max(round(speech_dict['start'] / sampling_rate, 1), 0)\n",
    "            speech_dict['end'] = min(round(speech_dict['end'] / sampling_rate, 1), audio_length_seconds)\n",
    "    elif step > 1:\n",
    "        for speech_dict in speeches:\n",
    "            speech_dict['start'] *= step\n",
    "            speech_dict['end'] *= step\n",
    "\n",
    "    if visualize_probs:\n",
    "        make_visualization(speech_probs, window_siwze / sampling_rate)\n",
    "\n",
    "    return speeches\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
